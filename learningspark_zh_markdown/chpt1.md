CHAPTER 1 Introduction to Data Analysis with Spark 
***
本章在顶层概述什么是Apach Spark。如果你已很熟悉 Aparch Spark 及其组件，请跳过此章前往第二章。

# 什么是 Apache Spark 

Apache Spark 是一个为*快速*和*通用*性能所设计的集群计算平台。

就运算速度而言， Spark 延续了流行的 MapReduce 模型从而有效的支援多类计算，包括交互式查询(interactive queries)和流式计算(stream processing)。计算速度对于处理大型数据而言极其重要，在交互式数据探究中等上一分钟或一小时意味着截然不同的分析结果。 Spark最主要的特征之一就是可以在内存中进行计算，同时对于一些更为复杂需要依赖磁盘读写的应用程序，这个系统也能提供优于 Mapreduce 的性能。

就通用性而言， Spark 在设计之初考虑到了现已有的各类分布式系统的任务需求，如 batch applications，迭代算法(iterative algorithm)，交互式查询(interactive queries)，以及流式计算(streaming)。在同一引擎下支持这些不同的任务，Spark可以简单且经济的*组装*数据分析生产过程中常用的各类处理类型。此外，Spark减轻了同时维护不同的运维工具所造成管理上的负担。

Spark 在设计上也考虑到了高度可被访问性，提供了简洁的API接口，支持Python，Java，Scala，SQL这些语言，具备丰富的内建库。同时也整合了其他大数据工具。例如，Spark可以在Hadoop集群上运行并且访问任何如Cassandra这样的Hadoop数据源。



# 统一的堆栈 (A unified Stack)

Spark 项目包含了多个相互密切整合的组件。Spark的核心是一个“计算引擎”负责计划、分发、监视多台工作机器(或称为*计算集群*)上执行的计算任务。因为Spark的核心引擎既能高速运算也兼顾通用性，它驱动了多种上层专用任务的组件，如：SQL或机器学习。 这些组件在设计上可以紧密的相互操作，让用户感到像软件项目中的库一样便利的将它们结合起来使用。

紧密整合的设计哲学带了很多好处。 首先，所有的库和堆栈中高层的组件将得益于低层的改善。比如，当Spark的核心引擎被加入一项优化， SQL 和机器学习库也会自动提高运算性能。 其次，堆栈运行所需要的代价已被最小化，不同于同时运行5--10个独立的软件系统，用户只需要部署和运行一个软件系统即可。这里的代价包括了系统部署、维护、测试、支持和其他方面的成本。这意味着每次新添一个组件到Spark的堆栈， Spark 的用户可以立即试用新装的组件。这就将试验一种新数据分析方式的代价从下载、部署、学习一个新软件变为更新Spark即可。

最终，紧密整合的最大好处在于无缝整合不同处理模型的可能。例如，你可以在Spark中从源代码写一个使用机器学习的进行实时分类的模型。同时别的分析师能够实时的通过诸如SQL的方式(如，用非结构化数据结合查询数据)查询结果数据。此外，资深数据工程师和数据科学家可以通过Python shell for ad hoc analysis 访问相同的数据。而与此同时，IT团队正在同一个系统上进行着运维。

在此图1-1简明的列出了Spark的各个组件。

insert fig 1-1 here.

图 1-1. Spark堆栈

## Spark 核心组件 (Spark Core)

Spark 核心组件涵盖了Spark的所有基本功能，包括 任务计划组件、内存管理、错误还原、与存储系统交互等。Spark核心组件也包含了一个它最主要的编程抽象概念——*弹性分布式数据集*(*resilient distributed datasets*, RDDs)，RDDs表示了一个可被并行操作的分布式计算节点的集合。Spark 核心组件提供了很多 API 用于建立和操纵这些集合。

## Spark SQL

> Spark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Language (HQL)—and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. Spark SQL was added to Spark in version 1.0.

> Shark was an older SQL-on-Spark project out of the University of California, Berkeley, that modified Apache Hive to run on Spark. It has now been replaced by Spark SQL to provide better integration with the Spark engine and language APIs.

Spark SQL 是 Spark 处理结构化数据的包。它使用SQL或Apache Hive变种的SQL(又被称为Hive查询语言，HQL )语句是查询数据，同时它也支持多种来源的数据，诸如 Hive tables、 Parquet 和 JSON。除了提供连接Spark的SQL接口，Spark SQL让开发者能够在 SQL 查询语句中加入Python、Java 、Scala编写的支持RDD的操控数据的程序，结合简单的应用程序，便使得SQL具备处理复杂分析任务的能力。Spark提供的这种和rich计算环境紧密的结合使Spark SQL与众不同于其他开源数据仓库工具。在Spark 1.0版本后Spark SQL被加入。

Shark 是 Spark 上支持 SQL 的早起开发计划，通过修改 Apache Hive 在Spark上运行，加州(伯克利)大学当时并未参与到此计划。目前它已经被 Spark SQL 这个能与Spark引擎和API更好整合的项目替代。

## Spark Streaming

> Spark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.

Spark Streaming 是Spark中运行数据流实时处理的组件。

## Spark MLlib

Spark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. It also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. All of these methods are designed to scale out across a cluster.

## GraphX

GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs (e.g.,  subgraph and mapVertices ) and a library of common graph algorithms (e.g., PageRank and triangle counting).

## Cluster Managers

Under the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the Standalone Scheduler. If you are just installing Spark on an empty set of machines, the Standalone Scheduler provides an easy way to get started; if you already have a Hadoop YARN or Mesos cluster, however, Spark’s support for these cluster managers allows your applications to also run on them. Chapter 7 explores the different options and how to choose the correct cluster manager.

## Who Uses Spark, and for What?

Because Spark is a general-purpose framework for cluster computing, it is used for a diverse range of applications. In the Preface we outlined two groups of readers that this book targets: data scientists and engineers. Let’s take a closer look at each group and how it uses Spark. Unsurprisingly, the typical use cases differ between the two,but we can roughly classify them into two categories, data science and data applications.

Of course, these are imprecise disciplines and usage patterns, and many folks have skills from both, sometimes playing the role of the investigating data scientist, and then “changing hats” and writing a hardened data processing application. Nonetheless, it can be illuminating to consider the two groups and their respective use cases separately.

## Data Science Tasks

Data science, a discipline that has been emerging over the past few years, centers on analyzing data. While there is no standard definition, for our purposes a data scientist is somebody whose main task is to analyze and model data. Data scientists may have experience with SQL, statistics, predictive modeling (machine learning), and programming, usually in Python, Matlab, or R. Data scientists also have experience with techniques necessary to transform data into formats that can be analyzed for insights (sometimes referred to as data wrangling).

Data scientists use their skills to analyze data with the goal of answering a question or discovering insights. Oftentimes, their workflow involves ad hoc analysis, so they use interactive shells (versus building complex applications) that let them see results of queries and snippets of code in the least amount of time. Spark’s speed and simple APIs shine for this purpose, and its built-in libraries mean that many algorithms are available out of the box.

Spark supports the different tasks of data science with a number of components. The Spark shell makes it easy to do interactive data analysis using Python or Scala. Spark SQL also has a separate SQL shell that can be used to do data exploration using SQL, or Spark SQL can be used as part of a regular Spark program or in the Spark shell. Machine learning and data analysis is supported through the MLLib libraries. In addition, there is support for calling out to external programs in Matlab or R. Spark enables data scientists to tackle problems with larger data sizes than they could before with tools like R or Pandas. 

Sometimes, after the initial exploration phase, the work of a data scientist will be “productized,” or extended, hardened (i.e., made fault-tolerant), and tuned to become a production data processing application, which itself is a component of a business application. For example, the initial investigation of a data scientist might lead to the creation of a production recommender system that is integrated into a
web application and used to generate product suggestions to users. Often it is a different person or team that leads the process of productizing the work of the data scientists, and that person is often an engineer.

## Data Processing Applications

The other main use case of Spark can be described in the context of the engineer persona. For our purposes here, we think of engineers as a large class of software developers who use Spark to build production data processing applications. These developers usually have an understanding of the principles of software engineering, such as encapsulation, interface design, and object-oriented programming. They frequently have a degree in computer science. They use their engineering skills to design and build software systems that implement a business use case.

For engineers, Spark provides a simple way to parallelize these applications across clusters, and hides the complexity of distributed systems programming, network communication, and fault tolerance. The system gives them enough control to monitor, inspect, and tune applications while allowing them to implement common tasks quickly. The modular nature of the API (based on passing distributed collections of objects) makes it easy to factor work into reusable libraries and test it locally.

Spark’s users choose to use it for their data processing applications because it provides a wide variety of functionality, is easy to learn and use, and is mature and reliable.

# A Brief History of Spark

Spark is an open source project that has been built and is maintained by a thriving and diverse community of developers. If you or your organization are trying Spark for the first time, you might be interested in the history of the project. Spark started in 2009 as a research project in the UC Berkeley RAD Lab, later to become the AMPLab. The researchers in the lab had previously been working on Hadoop MapReduce, and observed that MapReduce was inefficient for iterative and interactive computing jobs. Thus, from the beginning, Spark was designed to be fast for interactive queries and iterative algorithms, bringing in ideas like support for in-memory storage and efficient fault recovery.

Research papers were published about Spark at academic conferences and soon after its creation in 2009, it was already 10–20× faster than MapReduce for certain jobs.

Some of Spark’s first users were other groups inside UC Berkeley, including machine learning researchers such as the Mobile Millennium project, which used Spark to monitor and predict traffic congestion in the San Francisco Bay Area. In a very short time, however, many external organizations began using Spark, and today, over 50 organizations list themselves on the Spark PoweredBy page, and dozens speak about their use cases at Spark community events such as Spark Meetups and the Spark Summit. In addition to UC Berkeley, major contributors to Spark include Databricks, Yahoo!, and Intel.

In 2011, the AMPLab started to develop higher-level components on Spark, such as Shark (Hive on Spark) 1 and Spark Streaming. These and other components are sometimes referred to as the Berkeley Data Analytics Stack (BDAS).

Spark was first open sourced in March 2010, and was transferred to the Apache Software Foundation in June 2013, where it is now a top-level project.

# Spark Versions and Releases

Since its creation, Spark has been a very active project and community, with the number of contributors growing with each release. Spark 1.0 had over 100 individual contributors. Though the level of activity has rapidly grown, the community continues to release updated versions of Spark on a regular schedule. Spark 1.0 was released in May 2014. This book focuses primarily on Spark 1.1.0 and beyond, though most of the concepts and examples also work in earlier versions.

# Storage Layers for Spark

Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) or other storage systems supported by the Hadoop APIs (including your local filesystem, Amazon S3, Cassandra, Hive, HBase, etc.). It’s important to remember that Spark does not require Hadoop; it simply has support for storage systems implementing the Hadoop APIs. Spark supports text files, SequenceFiles, Avro, Parquet, and any other Hadoop InputFormat. We will look at interacting with these data sources in Chapter 5.


